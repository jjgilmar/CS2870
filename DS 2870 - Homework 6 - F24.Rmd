---
title: "Homework 6 - Objective Functions, Functions, and Loops"
author: "Joey Gilmartin"
date: "`r Sys.Date()`"
output: 
  html_document
---

##  Set up

```{r setup, echo = F, message=F}
knitr::opts_chunk$set(echo = T,
                      fig.align = "center")

# Load the tidyverse below
pacman::p_load(tidyverse)

# Read in the field goals.csv data set and save it as fg
fg <- read.csv("field_goals.csv")

```


## Logistic regression

**For our example of objective functions, we looked a simple linear regression since it is a very common machine learning method. Another common machine learning method is logistic regression, which attempts to estimate the probability of success of a binary (categorical with two outcome) variable.**

**While we won't be looking at logistic regression in this class, we'll use it as our example for this homework assignment!**

**The logistic regression model is:**

$$\log\left(\frac{p}{1-p}\right) = a + bx$$

**where $\log()$ is the natural log, $a$ is the intercept, and $b$ is the slope (like with linear regression!).**

**Note: The logistic regression model is shown above to show how it looks, you won't actually be using it in this assignment**

## Data description

**The field goals.csv data set is about the 4303 NFL field goal attempts for the 2020 - 2023 seasons. It has 2 columns:** 

1) **distance: The distance away the kicker was from the goal**

2) **result: A dummy variable if the kick was successful (1) or if it missed (0)**

**Our goal is to make a logistic regression model where we estimate the probability a field goal attempt is successful based on the distance of the attempt.**

\newpage

## Question 1: Logistic regression objective function

**Write a function for the objective function for logistic regression named `logit_of`. The objective function is:**

$$h(a, b) = \sum\left( \log\left(1 + e^{a + bx} \right) - y(a + bx) \right)$$

**The function will need 4 arguments:**

1) **x: A vector of the explanatory variable (predictor)**
2) **y: A vector of 1s and 0s representing the response variable**
3) **a: The chosen value of the intercept (default to 6.25 for later purposes)**
4) **b: The chosen value of the slope (default to 0)**

**How to calculate $e^x$ in R:  `exp(x)`**

```{r Q1 object fun}
logit_of <- function( x, y, a = 6.25, b = 0){
  #calculate obj value
  obj_value <- sum( log( 1 + exp(a + b*x) ) - y * ( a + b * x ) )
  
  return( obj_value )
}



```


Run the code chunks below and see if they match what is in Brightspace

```{r q1 test, echo = T}
# # a = 6.25, b = 0
logit_of(x = fg$distance, y = fg$result)
# 
# # a = 0, b = 0
logit_of(x = fg$distance, y = fg$result, a = 0)
# 
# # a = 6.25, b = -0.1
logit_of(x = fg$distance, y = fg$result, b = -0.1)

```

## Question 2: Grid search for the slope

**For question 2, you'll perform a grid search to find the best value of the slope, $b$, when we keep the intercept the same at $a = 6.25$**

### Part 2a: Data frame to save the results

**Create a data frame named logit_search that has 2 columns:**

1) **b_val: The different values of $b$ to be searched across. Start at -1, end at 1, and change by increments of 0.0001**

2) **of_val: The value of the objective function for the corresponding version of $b$** 

**Note: While you'll want to search over the range of -1 to +1 in increments of 0.0001 for your solutions, start just by searching over -1 to +1 by increments of 0.01 until you get your loop in question 2b working.**

```{r Q2a}
#create b_val and of_val vectors
b_val <-  seq(-1, 1, by = 0.0001)
of_val <- rep(-1, length(b_val))

#create df
logit_search <- data.frame( b_val = b_val, of_val = of_val )

#tibble to see results
tibble(logit_search)

```

\newpage

### Part 2b: Grid search for the values of $b$

**Conduct a grid search using the data frame created in part 2a for logistic regression.**

```{r Q2b}
#grid search
for ( i in 1: length(b_val) ){
  logit_search$of_val[i] <-  logit_of( x = fg$distance, 
                                       y = fg$result,
                                       b = b_val[i],
                                     )
}
#tibble
tibble(logit_search)

```

**Use the code chunk below to check that it worked by looking at the results in Brightspace**


```{r q2b check}
RNGversion("4.1.0"); set.seed(2870)
logit_search |> 
  slice_sample(n = 10) |> 
  arrange(b_val)
```

\newpage

### Part 2c: Graph of the grid search

**To ensure that we searched over enough values of the slope to find a true minimum, we graph the results. Create a line graph with the values of $b$ on the x-axis and the values of the objective function on the y-axis. Add a vertical red line (`geom_vline()`) at the minimum value and the value of the slope as text on the graph. See what the graph should look like on Brightspace.**

```{r q2c line graph}

min_logit_search <- logit_search |> 
  slice_min(
    order_by = of_val,
    n = 1
  )

min_logit_search

ggplot(
  data = logit_search,
  mapping = aes(
    x = b_val,
    y = of_val,
  )
) +
  geom_line(
    color = 'black',
    linewidth = .5
  ) +
  geom_vline(
    xintercept = min_logit_search$b_val,
    color = 'red',
    linetype = 'dashed'
  ) +
  annotate(
    "text",
    x = 0.1,
    y = 35000,
    label = paste("Slope = ", min_logit_search$b_val),
    color = 'red'
  ) +
  labs(
    x = "Slope",
    y = "Objective Function"
  ) + 
  theme_bw()
  


  
```


\newpage

## Question 3: Gradient Descent

### Gradient descent description

**A quicker alternative to a grid search is to perform gradient descent. It uses the value of the derivative to help find a better guess of the slope than the current one. When run well, it is much quicker than using a grid search.**

**How gradient descent works is by updating the current value ($b_0$) into the new value ($b_1$). The formula to update the value of the slope is:**

$$b_1 = b_0 - \alpha \times f'(b_0)$$

**where $\alpha$ is some predetermined value (we'll use 0.000001) and $f'(b_0)$ is the value of the derivative evaluated at $b_0$ (the derivative with the current value plugged in).**

**For example, let's try to find the minimum of $f(x) = x^2$. Let's say our current value of $x$ is $x_0 = 0.5$ and the derivative is $f'(x) = 2x$. Using $\alpha = 0.1$, we can find a better guess by:**

$$x_1 = x_0 - \alpha \times 2x_0 = 0.5 - (0.1)(2)(0.5) = 0.4$$

**which is closer to the actual minimum of 0. We'd repeat this process until the value of the objective function changes by a very small amount. Our stopping criteria will be:**

$$\left|\frac{f(x_1) - f(x_0)}{f(x_0)} \right| < c$$

**where $c$ is a number chosen before hand. If we choose c = 0.001, we'd check if we stop by**

$$\left|\frac{f(0.4) - f(0.5)}{f(0.5)} \right| = left|\frac{0.4^2 - 0.5^2}{0.5^2} \right|$$

$$\left|\frac{0.4^2 - 0.5^2}{0.5^2} \right| = 0.36$$

$$0.36  \gt 0.001$$

**Since $0.36 > 0.001$, we keep going and repeat and update our new guess by replacing $b_0$ with $b_1$:**

$$b_1 = 0.4 - 0.1(2)(0.4) = 0.32$$

**We keep going until the stopping condition is met**


**The code chunk below will create the function called `logit_der()` to calculate the derivative that you'll use to answer question 3:**

```{r Q3 derivative function, echo = T}
logit_der <- function(x, y, a = 6.25, b){
  return(-1 * sum(x * y - (x * exp(a + b * x))/(1 + exp(a + b * x))))
}

# Using the function:
logit_der(x = fg$distance, y = fg$result, a = 6.25, b = 0.5)
```

### Writing the gradient descent code

**Write the code to perform gradient descent below. For each loop, it should **

- update the number of iterations (number of loops). Call it `iters`. On the 5th loop, iters = 5, on the 10th loop, iters = 10, etc...

- calculate the value of the objective function with the current slope (call it `of_curr`)

- calculate the value of the derivative (call it `gradient`)

- update the value of the slope using gradient descent

- calculate the new value of the objective function (call it `of_new`)

You'll be using `alpha = 1e-6` and keep the intercept the same as in question 2 ($a = 6.25$)

```{r Q3 gradient descent set up, echo = T}
## objects need to perform gradient descent
iters <- 0 # Number of iterations for gradient descent
b <- 0.5   # Initial value of the slope

## Finding the value of the objective function with the current b
of_new <- logit_of(x = fg$distance, y = fg$result, a = 6.25, b = b)

of_curr <- 1  # Needed to get the loop started
alpha <- 1e-6 # how much the value of the slope changes based on the derivative
c <- 1e-5     # how much the derivative needs to change to stop the algorithm

```


**With the code chunk below, perform gradient descent**

```{r q3 running gradient descent}

while( abs(( of_new - of_curr) / of_curr ) > c){
  #updates iters 
  iters = iters + 1
  
  of_curr <- of_new
  
  #calculate gradient
  gradient <- logit_der(x = fg$distance, y = fg$result, a = 6.25, b = b)
  
  #update slope
  b <- b - alpha * gradient
  
  of_new <- logit_of(x = fg$distance, y = fg$result, a = 6.25, b = b)
}



```


**The code chunk below will check the results and that they match what is in Brightspace**

```{r Q3 check, echo = F}
c(
  "slope" = round(b, 4),
  "objective function" = round(of_new, 0),
  "iterations" = iters
)
```





